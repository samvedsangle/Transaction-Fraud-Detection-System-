{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Unzip and load the dataset\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Extracting dataset...\")\n",
        "with zipfile.ZipFile('PS_20174392719_1491204439457_log.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')\n",
        "\n",
        "print(\"âœ“ Extracted!\")\n",
        "\n",
        "print(\"\\nLoading dataset...\")\n",
        "df = pd.read_csv('PS_20174392719_1491204439457_log.csv')\n",
        "\n",
        "print(f\"âœ“ Dataset loaded: {df.shape}\")\n",
        "print(f\"âœ“ Fraud: {df['isFraud'].sum():,} ({df['isFraud'].mean()*100:.4f}%)\")\n",
        "print(\"\\nâœ“ Ready to run pipeline!\")\n",
        "\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "vH5T2sn4vE09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# COMPLETE FRAUD DETECTION PROJECT\n",
        "# ============================================================================\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import *\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import shap\n",
        "import plotly.express as px\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"FRAUD DETECTION - COMPLETE PIPELINE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nDataset: {len(df):,} transactions | Fraud: {df['isFraud'].sum():,} ({df['isFraud'].mean()*100:.4f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# [1/6] FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "print(\"\\n[1/6] Feature Engineering...\")\n",
        "df_feat = df.copy()\n",
        "\n",
        "df_feat['amount_log'] = np.log1p(df_feat['amount'])\n",
        "df_feat['amount_sqrt'] = np.sqrt(df_feat['amount'])\n",
        "df_feat['orig_balance_ratio'] = df_feat['amount'] / (df_feat['oldbalanceOrg'] + 1)\n",
        "df_feat['dest_balance_ratio'] = df_feat['amount'] / (df_feat['oldbalanceDest'] + 1)\n",
        "df_feat['is_amount_mismatch'] = (np.abs(df_feat['amount'] - (df_feat['oldbalanceOrg'] - df_feat['newbalanceOrig'])) > 0.01).astype(int)\n",
        "df_feat['is_orig_zero'] = (df_feat['oldbalanceOrg'] == 0).astype(int)\n",
        "df_feat['is_dest_zero'] = (df_feat['oldbalanceDest'] == 0).astype(int)\n",
        "df_feat['is_orig_drained'] = (df_feat['newbalanceOrig'] == 0).astype(int)\n",
        "df_feat['hour'] = df_feat['step'] % 24\n",
        "df_feat['day'] = df_feat['step'] // 24\n",
        "df_feat['type_encoded'] = LabelEncoder().fit_transform(df_feat['type'])\n",
        "type_dummies = pd.get_dummies(df_feat['type'], prefix='type')\n",
        "df_feat = pd.concat([df_feat, type_dummies], axis=1)\n",
        "df_feat['is_dest_merchant'] = df_feat['nameDest'].str.startswith('M').astype(int)\n",
        "df_feat['is_orig_merchant'] = df_feat['nameOrig'].str.startswith('M').astype(int)\n",
        "df_feat['amount_pct'] = df_feat.groupby('type')['amount'].transform(lambda x: x.rank(pct=True))\n",
        "\n",
        "print(f\"   âœ“ Created {df_feat.shape[1] - df.shape[1]} new features\")\n",
        "\n",
        "# ============================================================================\n",
        "# [2/6] PREPROCESSING\n",
        "# ============================================================================\n",
        "print(\"\\n[2/6] Preprocessing...\")\n",
        "df_proc = df_feat.drop(columns=['nameOrig', 'nameDest', 'isFlaggedFraud', 'type'], errors='ignore')\n",
        "X = df_proc.drop('isFraud', axis=1)\n",
        "y = df_proc['isFraud']\n",
        "X = X.replace([np.inf, -np.inf], np.nan).fillna(X.median())\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
        "\n",
        "print(f\"   âœ“ Features: {X.shape[1]} | Train: {len(X_train):,} | Test: {len(X_test):,}\")\n",
        "\n",
        "# ============================================================================\n",
        "# [3/6] SMOTE\n",
        "# ============================================================================\n",
        "print(\"\\n[3/6] Applying SMOTE (1-2 minutes)...\")\n",
        "smote = SMOTE(sampling_strategy=0.5, random_state=RANDOM_STATE)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
        "print(f\"   âœ“ Balanced: {len(X_train_balanced):,} samples\")\n",
        "\n",
        "# ============================================================================\n",
        "# [4/6] TRAIN MODELS\n",
        "# ============================================================================\n",
        "print(\"\\n[4/6] Training Models...\")\n",
        "models = {\n",
        "    'XGBoost': XGBClassifier(n_estimators=100, max_depth=6, random_state=RANDOM_STATE, eval_metric='logloss', verbosity=0),\n",
        "    'LightGBM': LGBMClassifier(n_estimators=100, max_depth=6, random_state=RANDOM_STATE, verbosity=-1),\n",
        "    'CatBoost': CatBoostClassifier(iterations=100, depth=6, random_state=RANDOM_STATE, verbose=0)\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_balanced, y_train_balanced)\n",
        "    print(f\"   âœ“ {name}\")\n",
        "\n",
        "# ============================================================================\n",
        "# [5/6] EVALUATE\n",
        "# ============================================================================\n",
        "print(\"\\n[5/6] Evaluating...\")\n",
        "results = []\n",
        "predictions = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred),\n",
        "        'Recall': recall_score(y_test, y_pred),\n",
        "        'F1-Score': f1_score(y_test, y_pred),\n",
        "        'ROC-AUC': roc_auc_score(y_test, y_proba)\n",
        "    })\n",
        "    predictions[name] = {'pred': y_pred, 'proba': y_proba}\n",
        "\n",
        "results_df = pd.DataFrame(results).sort_values('ROC-AUC', ascending=False)\n",
        "best_name = results_df.iloc[0]['Model']\n",
        "best_model = models[best_name]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))\n",
        "print(f\"\\nBest: {best_name} (ROC-AUC: {results_df.iloc[0]['ROC-AUC']:.4f})\")\n",
        "\n",
        "# Visualize\n",
        "fig = px.bar(results_df, x='Model', y=['Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
        "             title='Performance Comparison', barmode='group', height=500)\n",
        "fig.show()\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, predictions[best_name]['pred'])\n",
        "print(f\"\\nConfusion Matrix:\\n  TP: {cm[1,1]:,} | FN: {cm[1,0]:,}\\n  FP: {cm[0,1]:,} | TN: {cm[0,0]:,}\")\n",
        "\n",
        "# ============================================================================\n",
        "# [6/6] SHAP\n",
        "# ============================================================================\n",
        "print(\"\\n[6/6] SHAP Analysis...\")\n",
        "X_sample = X_train_balanced.sample(n=1000, random_state=RANDOM_STATE)\n",
        "explainer = shap.TreeExplainer(best_model)\n",
        "shap_values = explainer.shap_values(X_sample)\n",
        "if isinstance(shap_values, list):\n",
        "    shap_values = shap_values[1]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False, max_display=10)\n",
        "plt.title(f'Top Features - {best_name}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PROJECT COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\"\"\n",
        "Dataset: {len(df):,} transactions\n",
        "Best Model: {best_name}\n",
        "ROC-AUC: {results_df.iloc[0]['ROC-AUC']:.4f}\n",
        "Precision: {results_df.iloc[0]['Precision']:.4f}\n",
        "Recall: {results_df.iloc[0]['Recall']:.4f}\n",
        "\n",
        "Status: COMPLETE\n",
        "\"\"\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "-EFctQk9vMYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SAVE MODELS AND ARTIFACTS\n",
        "# ============================================================================\n",
        "import pickle\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SAVING MODELS AND ARTIFACTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create output directory\n",
        "!mkdir -p fraud_detection_models\n",
        "\n",
        "# 1. Save all trained models\n",
        "print(\"\\n[1/6] Saving trained models...\")\n",
        "for name, model in models.items():\n",
        "    filename = f\"fraud_detection_models/{name.replace(' ', '_').lower()}_model.pkl\"\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "    print(f\"   âœ“ {name} saved\")\n",
        "\n",
        "# 2. Save scaler\n",
        "print(\"\\n[2/6] Saving scaler...\")\n",
        "with open('fraud_detection_models/scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "print(\"   âœ“ Scaler saved\")\n",
        "\n",
        "# 3. Save feature names\n",
        "print(\"\\n[3/6] Saving feature names...\")\n",
        "with open('fraud_detection_models/feature_names.pkl', 'wb') as f:\n",
        "    pickle.dump(X_train.columns.tolist(), f)\n",
        "print(\"   âœ“ Feature names saved\")\n",
        "\n",
        "# 4. Save model results\n",
        "print(\"\\n[4/6] Saving model results...\")\n",
        "results_df.to_csv('fraud_detection_models/model_comparison.csv', index=False)\n",
        "print(\"   âœ“ Results CSV saved\")\n",
        "\n",
        "# 5. Save best model info\n",
        "print(\"\\n[5/6] Saving best model metadata...\")\n",
        "best_model_info = {\n",
        "    'model_name': best_name,\n",
        "    'roc_auc': float(results_df.iloc[0]['ROC-AUC']),\n",
        "    'precision': float(results_df.iloc[0]['Precision']),\n",
        "    'recall': float(results_df.iloc[0]['Recall']),\n",
        "    'f1_score': float(results_df.iloc[0]['F1-Score']),\n",
        "    'accuracy': float(results_df.iloc[0]['Accuracy']),\n",
        "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'dataset_size': len(df),\n",
        "    'features_count': X.shape[1],\n",
        "    'fraud_rate': float(df['isFraud'].mean())\n",
        "}\n",
        "\n",
        "with open('fraud_detection_models/best_model_info.json', 'w') as f:\n",
        "    json.dump(best_model_info, f, indent=4)\n",
        "print(\"   âœ“ Metadata saved\")\n",
        "\n",
        "# 6. Save confusion matrices\n",
        "print(\"\\n[6/6] Saving confusion matrices...\")\n",
        "cm_data = {}\n",
        "for name in models.keys():\n",
        "    cm = confusion_matrix(y_test, predictions[name]['pred'])\n",
        "    cm_data[name] = {\n",
        "        'true_negatives': int(cm[0,0]),\n",
        "        'false_positives': int(cm[0,1]),\n",
        "        'false_negatives': int(cm[1,0]),\n",
        "        'true_positives': int(cm[1,1])\n",
        "    }\n",
        "\n",
        "with open('fraud_detection_models/confusion_matrices.json', 'w') as f:\n",
        "    json.dump(cm_data, f, indent=4)\n",
        "print(\"   âœ“ Confusion matrices saved\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ“ ALL ARTIFACTS SAVED TO: fraud_detection_models/\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# List saved files\n",
        "print(\"\\nSaved files:\")\n",
        "!ls -lh fraud_detection_models/"
      ],
      "metadata": {
        "id": "U3Km8k2Ez57b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ADVANCED VISUALIZATIONS (FIXED)\n",
        "# ============================================================================\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"GENERATING ADVANCED VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# First, create confusion_matrices dictionary (was missing!)\n",
        "print(\"\\n[0/5] Creating confusion matrices for all models...\")\n",
        "confusion_matrices = {}\n",
        "for name in models.keys():\n",
        "    cm = confusion_matrix(y_test, predictions[name]['pred'])\n",
        "    confusion_matrices[name] = cm\n",
        "print(\"   âœ“ Confusion matrices created\")\n",
        "\n",
        "# 1. ROC Curves for All Models\n",
        "print(\"\\n[1/5] ROC Curves...\")\n",
        "fig = go.Figure()\n",
        "\n",
        "for name in models.keys():\n",
        "    y_proba = predictions[name]['proba']\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=fpr, y=tpr,\n",
        "        name=f'{name} (AUC={auc:.4f})',\n",
        "        mode='lines',\n",
        "        line=dict(width=2)\n",
        "    ))\n",
        "\n",
        "# Add diagonal\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[0, 1], y=[0, 1],\n",
        "    name='Random Classifier',\n",
        "    mode='lines',\n",
        "    line=dict(color='black', dash='dash', width=1)\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='ROC Curves - All Models',\n",
        "    xaxis_title='False Positive Rate',\n",
        "    yaxis_title='True Positive Rate',\n",
        "    height=600,\n",
        "    hovermode='closest'\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "# 2. Precision-Recall Curves\n",
        "print(\"\\n[2/5] Precision-Recall Curves...\")\n",
        "fig2 = go.Figure()\n",
        "\n",
        "for name in models.keys():\n",
        "    y_proba = predictions[name]['proba']\n",
        "    precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_proba)\n",
        "    avg_prec = average_precision_score(y_test, y_proba)\n",
        "\n",
        "    fig2.add_trace(go.Scatter(\n",
        "        x=recall_vals, y=precision_vals,\n",
        "        name=f'{name} (AP={avg_prec:.4f})',\n",
        "        mode='lines',\n",
        "        line=dict(width=2)\n",
        "    ))\n",
        "\n",
        "fig2.update_layout(\n",
        "    title='Precision-Recall Curves - All Models',\n",
        "    xaxis_title='Recall',\n",
        "    yaxis_title='Precision',\n",
        "    height=600,\n",
        "    hovermode='closest'\n",
        ")\n",
        "fig2.show()\n",
        "\n",
        "# 3. Confusion Matrices Grid\n",
        "print(\"\\n[3/5] Confusion Matrices Grid...\")\n",
        "fig3 = make_subplots(\n",
        "    rows=1, cols=3,\n",
        "    subplot_titles=[f'{name}' for name in models.keys()],\n",
        "    specs=[[{'type': 'heatmap'}, {'type': 'heatmap'}, {'type': 'heatmap'}]]\n",
        ")\n",
        "\n",
        "for idx, name in enumerate(models.keys(), 1):\n",
        "    cm = confusion_matrices[name]\n",
        "\n",
        "    fig3.add_trace(\n",
        "        go.Heatmap(\n",
        "            z=cm,\n",
        "            x=['Legitimate', 'Fraud'],\n",
        "            y=['Legitimate', 'Fraud'],\n",
        "            colorscale='Blues',\n",
        "            showscale=(idx==3),\n",
        "            text=cm,\n",
        "            texttemplate='%{text}',\n",
        "            textfont={\"size\": 14}\n",
        "        ),\n",
        "        row=1, col=idx\n",
        "    )\n",
        "\n",
        "fig3.update_layout(\n",
        "    title_text='Confusion Matrices - All Models',\n",
        "    height=400\n",
        ")\n",
        "fig3.show()\n",
        "\n",
        "# 4. Metrics Radar Chart\n",
        "print(\"\\n[4/5] Metrics Radar Chart...\")\n",
        "fig4 = go.Figure()\n",
        "\n",
        "metrics_to_plot = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'ROC-AUC']\n",
        "\n",
        "for name in models.keys():\n",
        "    model_data = results_df[results_df['Model'] == name].iloc[0]\n",
        "    values = [model_data[m] for m in metrics_to_plot]\n",
        "    values.append(values[0])  # Close the polygon\n",
        "\n",
        "    fig4.add_trace(go.Scatterpolar(\n",
        "        r=values,\n",
        "        theta=metrics_to_plot + [metrics_to_plot[0]],\n",
        "        fill='toself',\n",
        "        name=name\n",
        "    ))\n",
        "\n",
        "fig4.update_layout(\n",
        "    polar=dict(radialaxis=dict(visible=True, range=[0.9, 1.0])),\n",
        "    title='Model Performance Radar Chart',\n",
        "    height=600\n",
        ")\n",
        "fig4.show()\n",
        "\n",
        "# 5. Feature Importance Comparison\n",
        "print(\"\\n[5/5] Feature Importance (Top 15)...\")\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    feature_imp = pd.DataFrame({\n",
        "        'Feature': X_train.columns,\n",
        "        'Importance': best_model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False).head(15)\n",
        "\n",
        "    fig5 = px.bar(\n",
        "        feature_imp,\n",
        "        x='Importance',\n",
        "        y='Feature',\n",
        "        orientation='h',\n",
        "        title=f'Top 15 Feature Importances - {best_name}',\n",
        "        color='Importance',\n",
        "        color_continuous_scale='Viridis'\n",
        "    )\n",
        "    fig5.update_layout(height=600, yaxis={'categoryorder':'total ascending'})\n",
        "    fig5.show()\n",
        "\n",
        "print(\"\\nâœ“ All visualizations generated!\")"
      ],
      "metadata": {
        "id": "tr-lbEqF0it3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PRODUCTION-READY PREDICTION FUNCTION (FIXED)\n",
        "# ============================================================================\n",
        "\n",
        "def predict_fraud_with_explanation(transaction_data,\n",
        "                                   provide_explanation=True,\n",
        "                                   top_features=5):\n",
        "    \"\"\"\n",
        "    Predict fraud probability with optional SHAP explanation\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    transaction_data : dict or pd.DataFrame\n",
        "        Transaction features (must match training features)\n",
        "    provide_explanation : bool\n",
        "        Whether to generate SHAP explanation\n",
        "    top_features : int\n",
        "        Number of top contributing features to show\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Prediction results with explanation\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert to DataFrame if dict\n",
        "    if isinstance(transaction_data, dict):\n",
        "        transaction_df = pd.DataFrame([transaction_data])\n",
        "    else:\n",
        "        transaction_df = transaction_data.copy()\n",
        "\n",
        "    # Ensure correct feature order\n",
        "    transaction_df = transaction_df[X_train.columns]\n",
        "\n",
        "    # Scale\n",
        "    scaled_data = scaler.transform(transaction_df)\n",
        "    scaled_df = pd.DataFrame(scaled_data, columns=X_train.columns)\n",
        "\n",
        "    # Predict\n",
        "    prediction = best_model.predict(scaled_df)[0]\n",
        "    probability = best_model.predict_proba(scaled_df)[0]\n",
        "\n",
        "    # Determine risk level\n",
        "    fraud_prob = probability[1]\n",
        "    if fraud_prob > 0.8:\n",
        "        risk_level = \"CRITICAL\"\n",
        "    elif fraud_prob > 0.5:\n",
        "        risk_level = \"HIGH\"\n",
        "    elif fraud_prob > 0.3:\n",
        "        risk_level = \"MEDIUM\"\n",
        "    else:\n",
        "        risk_level = \"LOW\"\n",
        "\n",
        "    result = {\n",
        "        'prediction': 'FRAUD' if prediction == 1 else 'LEGITIMATE',\n",
        "        'fraud_probability': float(fraud_prob),\n",
        "        'legitimate_probability': float(probability[0]),\n",
        "        'risk_level': risk_level,\n",
        "        'confidence': float(max(probability)),\n",
        "        'model_used': best_name\n",
        "    }\n",
        "\n",
        "    # Add explanation if requested\n",
        "    if provide_explanation:\n",
        "        shap_values = explainer.shap_values(scaled_df)\n",
        "        if isinstance(shap_values, list):\n",
        "            shap_values = shap_values[1]\n",
        "\n",
        "        # Get top contributing features\n",
        "        feature_contributions = pd.DataFrame({\n",
        "            'feature': X_train.columns,\n",
        "            'shap_value': shap_values[0],\n",
        "            'abs_shap_value': np.abs(shap_values[0])\n",
        "        }).sort_values('abs_shap_value', ascending=False)\n",
        "\n",
        "        top_contrib = feature_contributions.head(top_features)[['feature', 'shap_value']].to_dict('records')\n",
        "\n",
        "        result['explanation'] = {\n",
        "            'top_contributing_features': top_contrib,\n",
        "            'total_features_analyzed': len(X_train.columns)\n",
        "        }\n",
        "\n",
        "    return result\n",
        "\n",
        "# Test the function\n",
        "print(\"=\"*80)\n",
        "print(\"TESTING PREDICTION FUNCTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test with a fraud case - FIXED indexing\n",
        "fraud_idx = y_test[y_test==1].index[0]\n",
        "fraud_sample = X_test.loc[[fraud_idx]]  # Use .loc instead of .iloc\n",
        "result_fraud = predict_fraud_with_explanation(fraud_sample, provide_explanation=True)\n",
        "\n",
        "print(\"\\n[TEST 1] Fraudulent Transaction:\")\n",
        "print(f\"  Prediction: {result_fraud['prediction']}\")\n",
        "print(f\"  Fraud Probability: {result_fraud['fraud_probability']:.2%}\")\n",
        "print(f\"  Risk Level: {result_fraud['risk_level']}\")\n",
        "print(f\"  Model: {result_fraud['model_used']}\")\n",
        "print(\"\\n  Top Contributing Features:\")\n",
        "for i, feat in enumerate(result_fraud['explanation']['top_contributing_features'], 1):\n",
        "    direction = \"â†’ Fraud\" if feat['shap_value'] > 0 else \"â†’ Legit\"\n",
        "    print(f\"    {i}. {feat['feature']:30s} {direction:12s} (impact: {abs(feat['shap_value']):.4f})\")\n",
        "\n",
        "# Test with legitimate case - FIXED indexing\n",
        "legit_idx = y_test[y_test==0].index[0]\n",
        "legit_sample = X_test.loc[[legit_idx]]  # Use .loc instead of .iloc\n",
        "result_legit = predict_fraud_with_explanation(legit_sample, provide_explanation=True)\n",
        "\n",
        "print(\"\\n[TEST 2] Legitimate Transaction:\")\n",
        "print(f\"  Prediction: {result_legit['prediction']}\")\n",
        "print(f\"  Fraud Probability: {result_legit['fraud_probability']:.2%}\")\n",
        "print(f\"  Risk Level: {result_legit['risk_level']}\")\n",
        "\n",
        "print(\"\\nâœ“ Prediction function created and tested successfully!\")"
      ],
      "metadata": {
        "id": "qDgjKyDy1-hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CREATING INTERACTIVE PREDICTION DEMO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def create_fraud_detection_demo():\n",
        "    \"\"\"Interactive demo for fraud prediction\"\"\"\n",
        "\n",
        "    output = widgets.Output()\n",
        "\n",
        "    # Create button\n",
        "    button = widgets.Button(\n",
        "        description=\"ðŸ” Analyze Random Transaction\",\n",
        "        button_style='primary',\n",
        "        layout=widgets.Layout(width='300px', height='50px')\n",
        "    )\n",
        "\n",
        "    # Transaction type filter\n",
        "    filter_dropdown = widgets.Dropdown(\n",
        "        options=['All', 'Fraudulent', 'Legitimate'],\n",
        "        value='All',\n",
        "        description='Filter:',\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    def on_button_click(b):\n",
        "        with output:\n",
        "            clear_output()\n",
        "\n",
        "            # Filter based on dropdown\n",
        "            if filter_dropdown.value == 'Fraudulent':\n",
        "                available_idx = y_test[y_test == 1].index\n",
        "            elif filter_dropdown.value == 'Legitimate':\n",
        "                available_idx = y_test[y_test == 0].index\n",
        "            else:\n",
        "                available_idx = y_test.index\n",
        "\n",
        "            if len(available_idx) == 0:\n",
        "                print(\"No transactions available\")\n",
        "                return\n",
        "\n",
        "            # Get random transaction\n",
        "            random_idx = np.random.choice(available_idx)\n",
        "            transaction = X_test.loc[[random_idx]]\n",
        "            actual_label = y_test.loc[random_idx]\n",
        "\n",
        "            # Predict\n",
        "            result = predict_fraud_with_explanation(transaction, provide_explanation=True, top_features=5)\n",
        "\n",
        "            # Display results\n",
        "            print(\"=\"*80)\n",
        "            print(f\"TRANSACTION ANALYSIS - ID: {random_idx}\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "            # Status\n",
        "            match_emoji = \"âœ“\" if (actual_label == 1 and result['prediction'] == 'FRAUD') or \\\n",
        "                               (actual_label == 0 and result['prediction'] == 'LEGITIMATE') else \"âœ—\"\n",
        "\n",
        "            print(f\"\\nACTUAL:     {'ðŸ”´ Fraudulent' if actual_label == 1 else 'ðŸŸ¢ Legitimate'}\")\n",
        "            print(f\"PREDICTED:  {'ðŸ”´ ' + result['prediction']}\")\n",
        "            print(f\"MATCH:      {match_emoji} {'Correct' if match_emoji == 'âœ“' else 'Incorrect'}\")\n",
        "\n",
        "            print(f\"\\nRISK ASSESSMENT:\")\n",
        "            print(f\"  Fraud Probability:  {result['fraud_probability']:.2%}\")\n",
        "            print(f\"  Risk Level:         {result['risk_level']}\")\n",
        "            print(f\"  Confidence:         {result['confidence']:.2%}\")\n",
        "            print(f\"  Model Used:         {result['model_used']}\")\n",
        "\n",
        "            print(f\"\\nTOP 5 CONTRIBUTING FACTORS:\")\n",
        "            for i, feat in enumerate(result['explanation']['top_contributing_features'], 1):\n",
        "                impact = \"Increases Fraud Risk\" if feat['shap_value'] > 0 else \"Decreases Fraud Risk\"\n",
        "                bar_length = int(abs(feat['shap_value']) * 20)\n",
        "                bar = \"â–ˆ\" * bar_length\n",
        "                print(f\"  {i}. {feat['feature'][:35]:35s}\")\n",
        "                print(f\"     {impact:25s} {bar} ({abs(feat['shap_value']):.4f})\")\n",
        "\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "            # Visualization\n",
        "            fig = go.Figure()\n",
        "\n",
        "            # Probability gauge\n",
        "            fig.add_trace(go.Indicator(\n",
        "                mode = \"gauge+number+delta\",\n",
        "                value = result['fraud_probability'] * 100,\n",
        "                title = {'text': \"Fraud Probability (%)\"},\n",
        "                delta = {'reference': 50},\n",
        "                gauge = {\n",
        "                    'axis': {'range': [None, 100]},\n",
        "                    'bar': {'color': \"darkred\" if result['fraud_probability'] > 0.5 else \"darkgreen\"},\n",
        "                    'steps': [\n",
        "                        {'range': [0, 30], 'color': \"lightgreen\"},\n",
        "                        {'range': [30, 50], 'color': \"lightyellow\"},\n",
        "                        {'range': [50, 80], 'color': \"orange\"},\n",
        "                        {'range': [80, 100], 'color': \"red\"}\n",
        "                    ],\n",
        "                    'threshold': {\n",
        "                        'line': {'color': \"black\", 'width': 4},\n",
        "                        'thickness': 0.75,\n",
        "                        'value': 50\n",
        "                    }\n",
        "                }\n",
        "            ))\n",
        "\n",
        "            fig.update_layout(height=400)\n",
        "            fig.show()\n",
        "\n",
        "    button.on_click(on_button_click)\n",
        "\n",
        "    # Display interface\n",
        "    display(HTML(\"<h2>ðŸŽ¯ Fraud Detection Interactive Demo</h2>\"))\n",
        "    display(HTML(\"<p>Click the button to analyze a random transaction from the test set</p>\"))\n",
        "    display(filter_dropdown)\n",
        "    display(button)\n",
        "    display(output)\n",
        "\n",
        "# Launch demo\n",
        "create_fraud_detection_demo()\n",
        "\n",
        "print(\"\\nâœ“ Interactive demo created!\")\n",
        "print(\"Click the button above to test predictions!\")"
      ],
      "metadata": {
        "id": "CWSt4kih2YDD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}